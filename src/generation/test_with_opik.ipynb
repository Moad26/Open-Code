{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline with Opik Tracking\n",
    "\n",
    "This notebook demonstrates how to integrate Opik for:\n",
    "- Prompt tracking and versioning\n",
    "- LLM call logging\n",
    "- RAG pipeline monitoring\n",
    "- Experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Opik if not already installed\n",
    "# !pip install opik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = str(Path.cwd().parent.parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "import ollama\n",
    "from src.utils.config import LLMConfig, settings\n",
    "from src.shared.models import SearchResult\n",
    "\n",
    "# Opik imports\n",
    "import opik\n",
    "from opik import track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Opik\n",
    "\n",
    "Configure Opik to track your experiments. You can use the cloud version or self-hosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: ========================\n",
      "The API key must be specified to log data to https://www.comet.com/opik.\n",
      "You can use `opik configure` CLI command to configure your environment for logging.\n",
      "See the configuration details in the docs: https://www.comet.com/docs/opik/tracing/sdk_configuration.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Opik initialized - Project: rag-pipeline\n"
     ]
    }
   ],
   "source": [
    "# Initialize Opik client\n",
    "# For cloud: opik.configure(api_key=\"your-api-key\")\n",
    "# For local: opik.configure(use_local=True)\n",
    "\n",
    "opik_client = opik.Opik()\n",
    "\n",
    "# Optional: Create or use existing project\n",
    "PROJECT_NAME = \"rag-pipeline\"\n",
    "print(f\"âœ… Opik initialized - Project: {PROJECT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Management with Opik\n",
    "\n",
    "Store and version your prompts using Opik's prompt management features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'prompt_template' from 'opik' (/home/moad/desktop/open-books/.venv/lib/python3.11/site-packages/opik/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create prompt templates with versioning\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopik\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prompt_template\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Query refinement prompt (v1)\u001b[39;00m\n\u001b[32m      5\u001b[39m QUERY_REFINEMENT_PROMPT_V1 = prompt_template(\n\u001b[32m      6\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mquery_refinement\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     version=\u001b[33m\"\u001b[39m\u001b[33mv1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \u001b[33mProvide 3 alternative phrasings, one per line:\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'prompt_template' from 'opik' (/home/moad/desktop/open-books/.venv/lib/python3.11/site-packages/opik/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Create prompt templates with versioning\n",
    "from opik import prompt_template\n",
    "\n",
    "# Query refinement prompt (v1)\n",
    "QUERY_REFINEMENT_PROMPT_V1 = prompt_template(\n",
    "    name=\"query_refinement\",\n",
    "    version=\"v1\",\n",
    "    template=\"\"\"Generate 3 different versions of this question for better document search.\n",
    "Original question: {question}\n",
    "\n",
    "Provide 3 alternative phrasings, one per line:\"\"\"\n",
    ")\n",
    "\n",
    "# Answer generation prompt (v1)\n",
    "ANSWER_GENERATION_PROMPT_V1 = prompt_template(\n",
    "    name=\"answer_generation\",\n",
    "    version=\"v1\",\n",
    "    template=\"\"\"Answer this question using only the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Prompt templates registered with Opik\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaManager:\n",
    "    @staticmethod\n",
    "    def ensure_ready(model_name: str = \"llama3.2\"):\n",
    "        try:\n",
    "            models = ollama.list()\n",
    "            if not any(model_name in m['name'] for m in models['models']):\n",
    "                print(f\" Downloading {model_name}...\")\n",
    "                ollama.pull(model_name)\n",
    "            print(f\" {model_name} ready\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"  Ollama not running!\")\n",
    "            print(f\"   Run: ollama serve\")\n",
    "            print(f\"   Then: ollama pull {model_name}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator with Opik Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGenerator(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        pass\n",
    "\n",
    "class OllamaGenerator(BaseGenerator):\n",
    "    def __init__(self, config: LLMConfig, auto_setup: bool = True):\n",
    "        self.model = config.model_name\n",
    "        self.temperature = config.temperature\n",
    "        if auto_setup:\n",
    "            OllamaManager.ensure_ready(self.model)\n",
    "    \n",
    "    @track(name=\"ollama_generation\", project_name=PROJECT_NAME)\n",
    "    def generate(self, prompt: str, metadata: dict = None) -> str:\n",
    "        \"\"\"Generate response with Opik tracking.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The input prompt\n",
    "            metadata: Additional metadata to log (e.g., prompt version, purpose)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Track the LLM call\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": self.temperature}\n",
    "            )\n",
    "            \n",
    "            result = response['message']['content']\n",
    "            \n",
    "            # Log additional metadata if provided\n",
    "            if metadata:\n",
    "                opik.track_metadata(metadata)\n",
    "            \n",
    "            # Log input/output\n",
    "            opik.log_traces(\n",
    "                input={\"prompt\": prompt, \"metadata\": metadata},\n",
    "                output={\"response\": result},\n",
    "                tags=[\"generation\", self.model]\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            opik.log_error(str(e))\n",
    "            return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Constructor with Prompt Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryConstructor(ABC):\n",
    "    @abstractmethod\n",
    "    def refine_query(self, query: str) -> list[str]:\n",
    "        pass\n",
    "\n",
    "class MultiQueryConstructor(QueryConstructor):\n",
    "    def __init__(self, generator: BaseGenerator, prompt_version: str = \"v1\") -> None:\n",
    "        self.generator = generator\n",
    "        self.prompt_version = prompt_version\n",
    "        # Use versioned prompt template\n",
    "        self.template = QUERY_REFINEMENT_PROMPT_V1\n",
    "\n",
    "    @track(name=\"query_refinement\", project_name=PROJECT_NAME)\n",
    "    def refine_query(self, query: str) -> list[str]:\n",
    "        \"\"\"Refine query with tracking.\"\"\"\n",
    "        # Format prompt using template\n",
    "        prompt = self.template.format(question=query)\n",
    "        \n",
    "        # Generate with metadata about prompt version\n",
    "        metadata = {\n",
    "            \"prompt_version\": self.prompt_version,\n",
    "            \"prompt_name\": \"query_refinement\",\n",
    "            \"original_query\": query\n",
    "        }\n",
    "        \n",
    "        response = self.generator.generate(prompt, metadata=metadata)\n",
    "        queries = [q.strip() for q in response.split('\\n') if q.strip() and len(q.strip()) > 10]\n",
    "        \n",
    "        refined_queries = [query] + queries[:3]\n",
    "        \n",
    "        # Log the refinement results\n",
    "        opik.log_traces(\n",
    "            input={\"original_query\": query},\n",
    "            output={\"refined_queries\": refined_queries, \"count\": len(refined_queries)},\n",
    "            tags=[\"query_refinement\", self.prompt_version]\n",
    "        )\n",
    "        \n",
    "        return refined_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Generator with Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseQueryAnswerer(ABC):\n",
    "    @abstractmethod\n",
    "    def answer(self, result_search: List[SearchResult], query: str) -> str:\n",
    "        pass\n",
    "\n",
    "class QueryAnswerer(BaseQueryAnswerer):\n",
    "    def __init__(self, generator: BaseGenerator, prompt_version: str = \"v1\") -> None:\n",
    "        self.generator = generator\n",
    "        self.prompt_version = prompt_version\n",
    "        # Use versioned prompt template\n",
    "        self.template = ANSWER_GENERATION_PROMPT_V1\n",
    "    \n",
    "    @track(name=\"answer_generation\", project_name=PROJECT_NAME)\n",
    "    def answer(self, result_search: List[SearchResult], query: str) -> str:\n",
    "        \"\"\"Generate answer with tracking.\"\"\"\n",
    "        if not result_search:\n",
    "            return \"No relevant documents found.\"\n",
    "        \n",
    "        # Prepare context\n",
    "        context_parts = [f\"[{i}] {r.content}\" for i, r in enumerate(result_search, 1)]\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Format prompt\n",
    "        prompt = self.template.format(context=context, question=query)\n",
    "        \n",
    "        # Generate with metadata\n",
    "        metadata = {\n",
    "            \"prompt_version\": self.prompt_version,\n",
    "            \"prompt_name\": \"answer_generation\",\n",
    "            \"num_context_chunks\": len(result_search),\n",
    "            \"context_sources\": [r.metadata.get('source', 'unknown') for r in result_search]\n",
    "        }\n",
    "        \n",
    "        answer = self.generator.generate(prompt, metadata=metadata)\n",
    "        \n",
    "        # Log the answer generation\n",
    "        opik.log_traces(\n",
    "            input={\n",
    "                \"query\": query,\n",
    "                \"num_chunks\": len(result_search)\n",
    "            },\n",
    "            output={\"answer\": answer},\n",
    "            tags=[\"answer_generation\", self.prompt_version]\n",
    "        )\n",
    "        \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline with Full Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAGPipeline:\n",
    "    def __init__(self, vector_store, answerer: BaseQueryAnswerer):\n",
    "        self.vector_store = vector_store\n",
    "        self.answerer = answerer\n",
    "    \n",
    "    @track(name=\"simple_rag_query\", project_name=PROJECT_NAME)\n",
    "    def query(self, query: str, top_k: int = 5) -> str:\n",
    "        \"\"\"Execute RAG query with tracking.\"\"\"\n",
    "        # Retrieve documents\n",
    "        results = self.vector_store.query([query], n_result=top_k)\n",
    "        \n",
    "        # Log retrieval\n",
    "        opik.log_traces(\n",
    "            input={\"query\": query, \"top_k\": top_k},\n",
    "            output={\"num_results\": len(results)},\n",
    "            tags=[\"retrieval\", \"simple_rag\"]\n",
    "        )\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.answerer.answer(results, query)\n",
    "        \n",
    "        return answer\n",
    "\n",
    "\n",
    "class MultiQueryRAGPipeline:\n",
    "    def __init__(self, query_constructor: QueryConstructor, vector_store, answerer: BaseQueryAnswerer):\n",
    "        self.query_constructor = query_constructor\n",
    "        self.vector_store = vector_store\n",
    "        self.answerer = answerer\n",
    "    \n",
    "    @track(name=\"multi_query_rag\", project_name=PROJECT_NAME)\n",
    "    def query(self, query: str, top_k: int = 5) -> str:\n",
    "        \"\"\"Execute multi-query RAG with full tracking.\"\"\"\n",
    "        # Refine query (tracked in query_constructor)\n",
    "        refined_queries = self.query_constructor.refine_query(query)\n",
    "        print(f\" Using {len(refined_queries)} query variations\")\n",
    "        \n",
    "        # Retrieve with all queries\n",
    "        all_results = self.vector_store.query(refined_queries, n_result=top_k)\n",
    "        print(f\" Found {len(all_results)} total results\")\n",
    "        \n",
    "        # Log retrieval metrics\n",
    "        opik.log_traces(\n",
    "            input={\n",
    "                \"original_query\": query,\n",
    "                \"refined_queries\": refined_queries,\n",
    "                \"top_k\": top_k\n",
    "            },\n",
    "            output={\n",
    "                \"num_results\": len(all_results),\n",
    "                \"num_queries_used\": len(refined_queries)\n",
    "            },\n",
    "            tags=[\"retrieval\", \"multi_query_rag\"]\n",
    "        )\n",
    "        \n",
    "        # Generate answer (tracked in answerer)\n",
    "        answer = self.answerer.answer(all_results, query)\n",
    "        \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator\n",
    "config = LLMConfig(model_name=\"llama3.2\", temperature=0.7)\n",
    "generator = OllamaGenerator(config)\n",
    "\n",
    "# Create answerer with prompt versioning\n",
    "answerer = QueryAnswerer(generator, prompt_version=\"v1\")\n",
    "\n",
    "print(\"âœ… Components initialized with Opik tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vector Store\n",
    "\n",
    "*(Use your existing vector store loading code)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing vector store code here\n",
    "# from src.ingestion.vector_store import ...\n",
    "# vector_store = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test simple RAG\n",
    "simple_rag = SimpleRAGPipeline(vector_store=vector_store, answerer=answerer)\n",
    "\n",
    "# This will be tracked in Opik\n",
    "answer = simple_rag.query(\"What is Word2Vec?\", top_k=3)\n",
    "print(\"\\nðŸ“ Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-query RAG\n",
    "query_constructor = MultiQueryConstructor(generator, prompt_version=\"v1\")\n",
    "multi_rag = MultiQueryRAGPipeline(\n",
    "    query_constructor=query_constructor,\n",
    "    vector_store=vector_store,\n",
    "    answerer=answerer\n",
    ")\n",
    "\n",
    "# This will track the entire pipeline\n",
    "answer = multi_rag.query(\"What is Word2Vec?\", top_k=5)\n",
    "print(\"\\nðŸ“ Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Tracking\n",
    "\n",
    "Run experiments with different prompt versions and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new prompt version for testing\n",
    "ANSWER_GENERATION_PROMPT_V2 = prompt_template(\n",
    "    name=\"answer_generation\",\n",
    "    version=\"v2\",\n",
    "    template=\"\"\"You are a helpful assistant. Answer the question using the context provided.\n",
    "Be concise and cite sources when possible.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Detailed Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Test both versions\n",
    "test_queries = [\n",
    "    \"What is Word2Vec?\",\n",
    "    \"How does Word2Vec work?\",\n",
    "    \"What are the applications of word embeddings?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Test with v1\n",
    "    answerer_v1 = QueryAnswerer(generator, prompt_version=\"v1\")\n",
    "    rag_v1 = SimpleRAGPipeline(vector_store=vector_store, answerer=answerer_v1)\n",
    "    answer_v1 = rag_v1.query(query, top_k=3)\n",
    "    \n",
    "    print(f\"\\nV1 Answer: {answer_v1[:200]}...\")\n",
    "    \n",
    "    # Note: You would create answerer_v2 with the new template\n",
    "    # This demonstrates how to track different versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results in Opik Dashboard\n",
    "\n",
    "After running your experiments, you can:\n",
    "\n",
    "1. **View traces**: See the full execution flow of each RAG query\n",
    "2. **Compare prompts**: Compare different versions side-by-side\n",
    "3. **Analyze performance**: Track latency, token usage, costs\n",
    "4. **Debug issues**: Inspect failed queries and errors\n",
    "5. **Track experiments**: Compare different configurations\n",
    "\n",
    "Access the dashboard at:\n",
    "- Cloud: https://www.comet.com/site/products/opik/\n",
    "- Local: http://localhost:5173 (if using local deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Custom Metrics and Feedback\n",
    "\n",
    "Add custom metrics to track quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik import score\n",
    "\n",
    "def evaluate_answer_quality(query: str, answer: str, expected_keywords: List[str]) -> float:\n",
    "    \"\"\"Simple keyword-based quality score.\"\"\"\n",
    "    found_keywords = sum(1 for kw in expected_keywords if kw.lower() in answer.lower())\n",
    "    return found_keywords / len(expected_keywords) if expected_keywords else 0.0\n",
    "\n",
    "# Example usage\n",
    "query = \"What is Word2Vec?\"\n",
    "answer = simple_rag.query(query, top_k=3)\n",
    "\n",
    "# Score the answer\n",
    "quality_score = evaluate_answer_quality(\n",
    "    query=query,\n",
    "    answer=answer,\n",
    "    expected_keywords=[\"vector\", \"word\", \"embedding\", \"neural\"]\n",
    ")\n",
    "\n",
    "# Log the score\n",
    "opik.log_metric(\n",
    "    name=\"answer_quality\",\n",
    "    value=quality_score,\n",
    "    tags=[\"evaluation\", \"keyword_match\"]\n",
    ")\n",
    "\n",
    "print(f\"Quality Score: {quality_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback Loop\n",
    "\n",
    "Collect user feedback on answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_feedback(trace_id: str, rating: int, comment: str = \"\"):\n",
    "    \"\"\"Collect user feedback on a specific answer.\"\"\"\n",
    "    opik.log_feedback(\n",
    "        trace_id=trace_id,\n",
    "        score=rating,\n",
    "        comment=comment\n",
    "    )\n",
    "\n",
    "# Example: After getting an answer, collect feedback\n",
    "# trace_id = opik.get_current_trace_id()\n",
    "# collect_feedback(trace_id, rating=4, comment=\"Good answer but could be more detailed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
