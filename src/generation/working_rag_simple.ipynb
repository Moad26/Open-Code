{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working RAG Pipeline\n",
    "## All Issues Fixed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = str(Path.cwd().parent.parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "import ollama\n",
    "from src.utils.config import LLMConfig, get_config\n",
    "from src.shared.models import SearchResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaManager:\n",
    "    @staticmethod\n",
    "    def ensure_ready(model_name: str = \"llama3.2\"):\n",
    "        try:\n",
    "            models = ollama.list()\n",
    "            if not any(model_name in m['name'] for m in models['models']):\n",
    "                print(f\"üì• Downloading {model_name}...\")\n",
    "                ollama.pull(model_name)\n",
    "            print(f\"‚úÖ {model_name} ready\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Ollama not running!\")\n",
    "            print(f\"   Run: ollama serve\")\n",
    "            print(f\"   Then: ollama pull {model_name}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGenerator(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        pass\n",
    "\n",
    "class OllamaGenerator(BaseGenerator):\n",
    "    def __init__(self, config: LLMConfig, auto_setup: bool = True):\n",
    "        self.model = config.model_name\n",
    "        self.temperature = config.temperature\n",
    "        if auto_setup:\n",
    "            OllamaManager.ensure_ready(self.model)\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": self.temperature}\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryConstructor(ABC):\n",
    "    @abstractmethod\n",
    "    def refine_query(self, query: str) -> list[str]:\n",
    "        pass\n",
    "\n",
    "class MultiQueryConstructor(QueryConstructor):\n",
    "    def __init__(self, generator: BaseGenerator) -> None:\n",
    "        self.generator = generator\n",
    "        self.template = \"\"\"Generate 3 different versions of this question for better document search.\n",
    "Original question: {question}\n",
    "\n",
    "Provide 3 alternative phrasings, one per line:\"\"\"\n",
    "\n",
    "    def refine_query(self, query: str) -> list[str]:\n",
    "        prompt = self.template.format(question=query)\n",
    "        response = self.generator.generate(prompt)\n",
    "        queries = [q.strip() for q in response.split('\\n') if q.strip() and len(q.strip()) > 10]\n",
    "        return [query] + queries[:3]  # Original + up to 3 variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseQueryAnswerer(ABC):\n",
    "    @abstractmethod\n",
    "    def answer(self, result_search: List[SearchResult], query: str) -> str:\n",
    "        pass\n",
    "\n",
    "class QueryAnswerer(BaseQueryAnswerer):\n",
    "    def __init__(self, generator: BaseGenerator) -> None:\n",
    "        self.generator = generator\n",
    "        self.template = \"\"\"Answer this question using only the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    def answer(self, result_search: List[SearchResult], query: str) -> str:\n",
    "        if not result_search:\n",
    "            return \"No relevant documents found.\"\n",
    "        \n",
    "        context_parts = [f\"[{i}] {r.content}\" for i, r in enumerate(result_search, 1)]\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        prompt = self.template.format(context=context, question=query)\n",
    "        return self.generator.generate(prompt).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline (SIMPLE VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion.vector_store.stores import ChromaStore\n",
    "\n",
    "class SimpleRAGPipeline:\n",
    "    def __init__(self, vector_store: ChromaStore, answerer: BaseQueryAnswerer):\n",
    "        self.vector_store = vector_store\n",
    "        self.answerer = answerer\n",
    "    \n",
    "    def query(self, user_query: str, top_k: int = 5) -> str:\n",
    "        print(f\"üîç Searching for: {user_query}\")\n",
    "        \n",
    "        # Simple single query\n",
    "        results = self.vector_store.query_flattened([user_query], n_result=top_k)\n",
    "        \n",
    "        print(f\"üìÑ Found {len(results)} results\")\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.answerer.answer(results, user_query)\n",
    "        return answer\n",
    "\n",
    "\n",
    "class MultiQueryRAGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_constructor: QueryConstructor,\n",
    "        vector_store: ChromaStore, \n",
    "        answerer: BaseQueryAnswerer\n",
    "    ):\n",
    "        self.query_constructor = query_constructor\n",
    "        self.vector_store = vector_store\n",
    "        self.answerer = answerer\n",
    "    \n",
    "    def query(self, user_query: str, top_k: int = 5) -> str:\n",
    "        # Generate query variations\n",
    "        queries = self.query_constructor.refine_query(user_query)\n",
    "        print(f\"üîç Using {len(queries)} query variations\")\n",
    "        \n",
    "        # Search with all variations\n",
    "        results = self.vector_store.query(queries, n_result=top_k)\n",
    "        \n",
    "        print(f\"üìÑ Found {len(results)} total results\")\n",
    "        \n",
    "        # Take top results\n",
    "        top_results = results[:top_k]\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.answerer.answer(top_results, user_query)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Simple Version First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion.parsers.get_parser import get_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "pdf_path = Path(\"../../data/Word2Vec.pdf\")\n",
    "\n",
    "# Verify it exists\n",
    "import os\n",
    "\n",
    "print(f\"File exists: {os.path.exists(pdf_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion.chunking.get_chunker import get_chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = get_chunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 23:21:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.parsers.parsers\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mStarting to parse PDF: ../../data/Word2Vec.pdf\u001b[0m\n",
      "\u001b[32m2026-01-29 23:21:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.parsers.parsers\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mDocument converted successfully: 12 pages\u001b[0m\n",
      "\u001b[32m2026-01-29 23:21:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.parsers.parsers\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mStructure extracted: 23 chapters\u001b[0m\n",
      "\u001b[32m2026-01-29 23:21:19\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.ingestion.parsers.parsers\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m66\u001b[0m - \u001b[32m\u001b[1mSuccessfully parsed Word2Vec.pdf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "parsed_doc = parser.parse(pdf_path=pdf_path)\n",
    "chunked_doc = chunker.chunk(parsed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion.embedding.get_embbedder import get_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 23:22:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mLoading SentenceTransformer model: all-MiniLM-L6-v2\u001b[0m\n",
      "\u001b[32m2026-01-29 23:22:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mModel loaded: 384d on cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embedder = get_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 23:22:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.base_embed\u001b[0m:\u001b[36membed_chunk\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mSuccessfully embedded 33 chunks\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedder.embed_chunk(chunks=chunked_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 23:22:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mcreating or getting the collection\u001b[0m\n",
      "\u001b[32m2026-01-29 23:22:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mgetting the embedder\u001b[0m\n",
      "\u001b[32m2026-01-29 23:22:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mLoading SentenceTransformer model: all-MiniLM-L6-v2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing...\n",
      "‚ö†Ô∏è  Ollama not running!\n",
      "   Run: ollama serve\n",
      "   Then: ollama pull llama3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 23:22:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mModel loaded: 384d on cpu\u001b[0m\n",
      "\u001b[32m2026-01-29 23:22:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1madding chunks to the collection\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Simple RAG ready!\n"
     ]
    }
   ],
   "source": [
    "from src.utils.config import settings\n",
    "\n",
    "print(\"üöÄ Initializing...\")\n",
    "\n",
    "# Create components\n",
    "generator = OllamaGenerator(settings.llm, auto_setup=True)\n",
    "answerer = QueryAnswerer(generator)\n",
    "vector_store = ChromaStore(settings.vector_store)\n",
    "vector_store.ingest(embch=embeddings)\n",
    "# Simple pipeline (no query enhancement)\n",
    "simple_rag = SimpleRAGPipeline(\n",
    "    vector_store=vector_store,\n",
    "    answerer=answerer\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Simple RAG ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 23:22:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery_flattened\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mquerying the results\u001b[0m\n",
      "\u001b[32m2026-01-29 23:22:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery_flattened\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mfinished the querying\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SIMPLE RAG TEST\n",
      "============================================================\n",
      "üîç Searching for: What is Word2Vec?\n",
      "üìÑ Found 3 results\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SearchResult' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSIMPLE RAG TEST\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m answer = \u001b[43msimple_rag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is Word2Vec?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìù Answer:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mSimpleRAGPipeline.query\u001b[39m\u001b[34m(self, user_query, top_k)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìÑ Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m results\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Generate answer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43manswerer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mQueryAnswerer.answer\u001b[39m\u001b[34m(self, result_search, query)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result_search:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNo relevant documents found.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m context_parts = \u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m] \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult_search\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     23\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(context_parts)\n\u001b[32m     24\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m.template.format(context=context, question=query)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result_search:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNo relevant documents found.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m context_parts = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result_search, \u001b[32m1\u001b[39m)]\n\u001b[32m     23\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(context_parts)\n\u001b[32m     24\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m.template.format(context=context, question=query)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SearchResult' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "# Test simple version\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIMPLE RAG TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "answer = simple_rag.query(\"What is Word2Vec?\", top_k=3)\n",
    "\n",
    "print(\"\\nüìù Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Multi-Query Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-query RAG ready!\n"
     ]
    }
   ],
   "source": [
    "# Multi-query pipeline (with query enhancement)\n",
    "query_constructor = MultiQueryConstructor(generator)\n",
    "\n",
    "multi_rag = MultiQueryRAGPipeline(\n",
    "    query_constructor=query_constructor,\n",
    "    vector_store=vector_store,\n",
    "    answerer=answerer\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Multi-query RAG ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MULTI-QUERY RAG TEST\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 23:18:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mquering the results\u001b[0m\n",
      "\u001b[32m2026-01-29 23:18:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mfinished the quering\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Using 4 query variations\n",
      "üìÑ Found 4 total results\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMULTI-QUERY RAG TEST\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m answer = \u001b[43mmulti_rag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is Word2Vec?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìù Answer:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mMultiQueryRAGPipeline.query\u001b[39m\u001b[34m(self, user_query, top_k)\u001b[39m\n\u001b[32m     43\u001b[39m top_results = results[:top_k]\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Generate answer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43manswerer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mQueryAnswerer.answer\u001b[39m\u001b[34m(self, result_search, query)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result_search:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNo relevant documents found.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m context_parts = \u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m] \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult_search\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     23\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(context_parts)\n\u001b[32m     24\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m.template.format(context=context, question=query)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result_search:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNo relevant documents found.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m context_parts = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result_search, \u001b[32m1\u001b[39m)]\n\u001b[32m     23\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(context_parts)\n\u001b[32m     24\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m.template.format(context=context, question=query)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "# Test multi-query version\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-QUERY RAG TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "answer = multi_rag.query(\"What is Word2Vec?\", top_k=5)\n",
    "\n",
    "print(\"\\nüìù Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: Check what's in the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vector store has {vector_store.count()} documents\")\n",
    "\n",
    "# Try a simple search\n",
    "test_results = vector_store.query([\"word embedding\"], n_result=2)\n",
    "print(f\"\\nFound {len(test_results)} results for 'word embedding'\")\n",
    "\n",
    "if test_results:\n",
    "    print(\"\\nFirst result preview:\")\n",
    "    print(test_results[0].content[:200] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
