{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3159e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# This points to /desktop/open-books/\n",
    "project_root = str(Path.cwd().parent.parent)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Now 'from src.ingestion...' will work because 'src' is a folder inside project_root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2885746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from src.utils.config import LLMConfig\n",
    "from openai import OpenAI\n",
    "import ollama  # pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a90718f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' class OllamaGenerator(BaseGenerator):\\n    def __init__(self, config: LLMConfig):\\n        self.model = config.model_name\\n        self.temperature = config.temperature\\n        self.host = config.base_url or \"http://localhost:11434\"\\n\\n    def generate(self, prompt: str) -> str:\\n        response = ollama.chat(\\n            model=self.model,\\n            messages=[{\"role\": \"user\", \"content\": prompt}],\\n            options={\\n                \"temperature\": self.temperature,\\n            }\\n        )\\n        return response[\\'message\\'][\\'content\\'] '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class BaseGenerator(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Takes a formatted prompt and returns the LLM's text response.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class OpenAIGenerator(BaseGenerator):\n",
    "    def __init__(self, config: LLMConfig):\n",
    "        self.client = OpenAI(api_key=config.api_key)\n",
    "        self.model = config.model_name\n",
    "        self.temperature = config.temperature\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" class OllamaGenerator(BaseGenerator):\n",
    "    def __init__(self, config: LLMConfig):\n",
    "        self.model = config.model_name\n",
    "        self.temperature = config.temperature\n",
    "        self.host = config.base_url or \"http://localhost:11434\"\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        response = ollama.chat(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\n",
    "                \"temperature\": self.temperature,\n",
    "            }\n",
    "        )\n",
    "        return response['message']['content'] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e3df03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Ollama not running. Please start it:\n",
      "   Run in terminal: ollama serve\n"
     ]
    }
   ],
   "source": [
    "from src.utils.config import LLMConfig\n",
    "\n",
    "class OllamaManager:\n",
    "    @staticmethod\n",
    "    def ensure_ready(model_name: str = \"llama3.2\"):\n",
    "        \"\"\"One-liner to ensure Ollama + model are ready\"\"\"\n",
    "        try:\n",
    "            # Try to list models (checks if server is running)\n",
    "            models = ollama.list()\n",
    "            \n",
    "            # Check if model exists\n",
    "            if not any(model_name in m['name'] for m in models['models']):\n",
    "                print(f\"ðŸ“¥ Downloading {model_name}...\")\n",
    "                ollama.pull(model_name)\n",
    "            \n",
    "            print(f\"âœ… Ready to use {model_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Ollama not running. Please start it:\")\n",
    "            print(f\"   Run in terminal: ollama serve\")\n",
    "            return False\n",
    "\n",
    "# Usage - just one line!\n",
    "OllamaManager.ensure_ready(\"llama3.2\")\n",
    "class OllamaGenerator(BaseGenerator):\n",
    "    def __init__(self, config: LLMConfig, auto_setup: bool = True):\n",
    "        self.model = config.model_name\n",
    "        self.temperature = config.temperature\n",
    "        \n",
    "        # Auto-check if requested\n",
    "        if auto_setup:\n",
    "            OllamaManager.ensure_ready(self.model)\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": self.temperature}\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}. Is Ollama running? Run 'ollama serve'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d53b91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryConstructor(ABC):\n",
    "    @abstractmethod\n",
    "    def refine_query(self, query: str) -> list[str]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbbeaf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryConstructor(QueryConstructor):\n",
    "    def __init__(self,generator: BaseGenerator) -> None:\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "        different versions of the given user question to retrieve relevant documents from a vector \n",
    "        database. By generating multiple perspectives on the user question, your goal is to help\n",
    "        the user overcome some of the limitations of the distance-based similarity search. \n",
    "        Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "    def refine_query(self, query: str) -> list[str]:\n",
    "        prompt = self.template.format(question=query)\n",
    "        response = self.generator.generate(prompt)\n",
    "        \n",
    "        queries = [q.strip() for q in response.split('\\n') if q.strip()]\n",
    "        \n",
    "        return [query] + queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02bbde35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.shared.models import SearchResult\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22fcb29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseQueryAnswerer(ABC):\n",
    "    @abstractmethod\n",
    "    def answer(self,result_search:List[SearchResult],query:str)->str:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7792a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "from shared.models import SearchResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b8c44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryAnswerer(BaseQueryAnswerer):\n",
    "    def __init__(self, generator: BaseGenerator) -> None:\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.template = \"\"\"You are a helpful assistant answering questions based on provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Answer based only on the provided context\n",
    "- If the context doesn't contain enough information, say so\n",
    "- Be concise but complete\n",
    "- Cite relevant parts of the context when possible\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    def answer(self, result_search: List[SearchResult], query: str) -> str:\n",
    "        if not result_search:\n",
    "            return \"No relevant documents found to answer this question.\"\n",
    "        \n",
    "        context_parts = []\n",
    "        for i, result in enumerate(result_search, 1):\n",
    "            context_parts.append(f\"[Document {i}]\\n{result.content}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        prompt = self.template.format(context=context, question=query)\n",
    "        answer = self.generator.generate(prompt)\n",
    "        \n",
    "        return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb1735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryAnswererWithSources(BaseQueryAnswerer):\n",
    "    def __init__(self, generator: BaseGenerator) -> None:\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.template = \"\"\"Answer the question based on the context below. When using information from a document, reference it by its number [1], [2], etc.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide your answer with citations:\"\"\"\n",
    "    \n",
    "    def answer(self, result_search: List[SearchResult], query: str) -> str:\n",
    "        if not result_search:\n",
    "            return \"No relevant documents found.\"\n",
    "        \n",
    "        context_parts = []\n",
    "        for i, result in enumerate(result_search, 1):\n",
    "            metadata = f\"Source: {result.metadata.source_doc_title}\" if hasattr(result, 'metadata') else \"\"\n",
    "            context_parts.append(f\"[{i}] {result.content}\\n{metadata}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        prompt = self.template.format(context=context, question=query)\n",
    "        \n",
    "        return self.generator.generate(prompt).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e85089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moad/desktop/open-books/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.ingestion.vector_store.stores import ChromaStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1db1f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_constructor: QueryConstructor,\n",
    "        vector_store :ChromaStore, \n",
    "        answerer: BaseQueryAnswerer\n",
    "    ):\n",
    "        self.query_constructor = query_constructor\n",
    "        self.vector_store = vector_store\n",
    "        self.answerer = answerer\n",
    "    \n",
    "    def query(self, user_query: str, top_k: int = 5) -> str:\n",
    "        enhanced_queries = self.query_constructor.refine_query(user_query)\n",
    "        \n",
    "        all_results = []\n",
    "        results = self.vector_store.query(enhanced_queries,n_result=top_k)\n",
    "        all_results.extend(results)\n",
    "        \n",
    "        top_results = all_results[:top_k]\n",
    "        \n",
    "        answer = self.answerer.answer(top_results, user_query)\n",
    "        \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d8f32d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Ollama not running. Please start it:\n",
      "   Run in terminal: ollama serve\n",
      "Error: model 'gpt-4' not found (status code: 404). Is Ollama running? Run 'ollama serve'\n"
     ]
    }
   ],
   "source": [
    "# Just run this - it handles everything!\n",
    "from src.utils.config import settings\n",
    "\n",
    "\n",
    "\n",
    "# This auto-checks Ollama and downloads model if needed\n",
    "generator = OllamaGenerator(settings.llm, auto_setup=True)\n",
    "\n",
    "# Use it!\n",
    "response = generator.generate(\"Explain RAG in simple terms\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d316d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 22:43:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mcreating or getting the collection\u001b[0m\n",
      "\u001b[32m2026-01-29 22:43:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mgetting the embedder\u001b[0m\n",
      "\u001b[32m2026-01-29 22:43:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mLoading SentenceTransformer model: all-MiniLM-L6-v2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Ollama not running. Please start it:\n",
      "   Run in terminal: ollama serve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 22:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mModel loaded: 384d on cpu\u001b[0m\n",
      "\u001b[32m2026-01-29 22:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mquering the results\u001b[0m\n",
      "\u001b[32m2026-01-29 22:43:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mfinished the quering\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      8\u001b[39m vector_store = ChromaStore(settings.vector_store)\n\u001b[32m     10\u001b[39m rag = RAGPipeline(\n\u001b[32m     11\u001b[39m     query_constructor=query_constructor,\n\u001b[32m     12\u001b[39m     vector_store=vector_store,\n\u001b[32m     13\u001b[39m     answerer=answerer\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m answer = \u001b[43mrag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat are the main themes in the book?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mRAGPipeline.query\u001b[39m\u001b[34m(self, user_query, top_k)\u001b[39m\n\u001b[32m     17\u001b[39m all_results.extend(results)\n\u001b[32m     19\u001b[39m top_results = all_results[:top_k]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43manswerer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mQueryAnswerer.answer\u001b[39m\u001b[34m(self, result_search, query)\u001b[39m\n\u001b[32m     24\u001b[39m context_parts = []\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result_search, \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     context_parts.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Document \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(context_parts)\n\u001b[32m     30\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m.template.format(context=context, question=query)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "from src.utils.config import settings\n",
    "\n",
    "\n",
    "generator = OllamaGenerator(settings.llm)\n",
    "\n",
    "query_constructor = MultiQueryConstructor(generator)\n",
    "answerer = QueryAnswerer(generator)\n",
    "vector_store = ChromaStore(settings.vector_store)\n",
    "\n",
    "rag = RAGPipeline(\n",
    "    query_constructor=query_constructor,\n",
    "    vector_store=vector_store,\n",
    "    answerer=answerer\n",
    ")\n",
    "\n",
    "answer = rag.query(\"What are the main themes in the book?\", top_k=5)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
