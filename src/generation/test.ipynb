{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working RAG Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = str(Path.cwd().parent.parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "import ollama\n",
    "from src.utils.config import LLMConfig, settings\n",
    "from src.shared.models import SearchResult\n",
    "from src.utils.logger import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaManager:\n",
    "    @staticmethod\n",
    "    def ensure_ready(model_name: str = \"llama3.2\"):\n",
    "        try:\n",
    "            models = ollama.list()\n",
    "            if not any(model_name in m['name'] for m in models['models']):\n",
    "                logger.info(f\" Downloading {model_name}...\")\n",
    "                ollama.pull(model_name)\n",
    "            logger.info(f\" {model_name} ready\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.info(f\"  Ollama not running!\")\n",
    "            logger.info(f\"   Run: ollama serve\")\n",
    "            logger.info(f\"   Then: ollama pull {model_name}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGenerator(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        pass\n",
    "\n",
    "class OllamaGenerator(BaseGenerator):\n",
    "    def __init__(self, config: LLMConfig, auto_setup: bool = True):\n",
    "        self.model = config.model_name\n",
    "        self.temperature = config.temperature\n",
    "        if auto_setup:\n",
    "            OllamaManager.ensure_ready(self.model)\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": self.temperature}\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryConstructor(ABC):\n",
    "    @abstractmethod\n",
    "    def refine_query(self, query: str) -> list[str]:\n",
    "        pass\n",
    "\n",
    "class MultiQueryConstructor(QueryConstructor):\n",
    "    def __init__(self, generator: BaseGenerator) -> None:\n",
    "        self.generator = generator\n",
    "        self.template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries), one per line:\"\"\"\n",
    "\n",
    "    def refine_query(self, query: str) -> list[str]:\n",
    "        prompt = self.template.format(question=query)\n",
    "        response = self.generator.generate(prompt)\n",
    "        queries = [q.strip() for q in response.split('\\n') if q.strip() and len(q.strip()) > 10]\n",
    "        return [query] + queries[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseQueryAnswerer(ABC):\n",
    "    @abstractmethod\n",
    "    def answer(self, result_search: List[SearchResult], query: str) -> str:\n",
    "        pass\n",
    "\n",
    "class QueryAnswerer(BaseQueryAnswerer):\n",
    "    def __init__(self, generator: BaseGenerator) -> None:\n",
    "        self.generator = generator\n",
    "        self.template = \"\"\"Answer this question using only the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    def answer(self, result_search: List[SearchResult], query: str) -> str:\n",
    "        if not result_search:\n",
    "            return \"No relevant documents found.\"\n",
    "        \n",
    "        context_parts = [f\"[{i}] {r.content}\" for i, r in enumerate(result_search, 1)]\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        print(context)\n",
    "        prompt = self.template.format(context=context, question=query)\n",
    "        return self.generator.generate(prompt).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline (SIMPLE VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moad/desktop/open-books/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.ingestion.vector_store.stores import ChromaStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAGPipeline:\n",
    "    def __init__(self, vector_store: ChromaStore, answerer: BaseQueryAnswerer):\n",
    "        self.vector_store = vector_store\n",
    "        self.answerer = answerer\n",
    "    \n",
    "    def query(self, user_query: str, top_k: int = 5) -> str:\n",
    "        print(f\"üîç Searching for: {user_query}\")\n",
    "        \n",
    "        results = self.vector_store.query([user_query], n_result=top_k)\n",
    "        print(results)\n",
    "        print(type(results))\n",
    "        print(type(results[0]))\n",
    "        \n",
    "        print(f\" Found {len(results)} results\")\n",
    "        \n",
    "        answer = self.answerer.answer(results, user_query)\n",
    "        return answer\n",
    "\n",
    "\n",
    "class MultiQueryRAGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_constructor: QueryConstructor,\n",
    "        vector_store: ChromaStore, \n",
    "        answerer: BaseQueryAnswerer\n",
    "    ):\n",
    "        self.query_constructor = query_constructor\n",
    "        self.vector_store = vector_store\n",
    "        self.answerer = answerer\n",
    "    \n",
    "    def query(self, user_query: str, top_k: int = 10) -> str:\n",
    "        queries = self.query_constructor.refine_query(user_query)\n",
    "        print(f\" Using {len(queries)} query variations\")\n",
    "        print(queries)\n",
    "        \n",
    "        results = self.vector_store.query(queries, n_result=top_k)\n",
    "        \n",
    "        print(f\" Found {len(results)} total results\")\n",
    "        \n",
    "        top_results = results[:top_k]\n",
    "        \n",
    "        answer = self.answerer.answer(top_results, user_query)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Simple Version First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion.parsers.get_parser import get_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "pdf_path = Path(\"../../data/Word2Vec.pdf\")\n",
    "\n",
    "import os\n",
    "\n",
    "print(f\"File exists: {os.path.exists(pdf_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion.chunking.get_chunker import get_chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = get_chunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 14:13:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.parsers.parsers\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mStarting to parse PDF: ../../data/Word2Vec.pdf\u001b[0m\n",
      "\u001b[32m2026-01-31 14:13:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.parsers.parsers\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mDocument converted successfully: 12 pages\u001b[0m\n",
      "\u001b[32m2026-01-31 14:13:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.parsers.parsers\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mStructure extracted: 23 chapters\u001b[0m\n",
      "\u001b[32m2026-01-31 14:13:46\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.ingestion.parsers.parsers\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m69\u001b[0m - \u001b[32m\u001b[1mSuccessfully parsed Word2Vec.pdf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "parsed_doc = parser.parse(pdf_path=pdf_path)\n",
    "chunked_doc = chunker.chunk(parsed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion.embedding.get_embbedder import get_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 14:13:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mLoading SentenceTransformer model: all-MiniLM-L6-v2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 14:13:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mModel loaded: 384d on cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embedder = get_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 14:13:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.base_embed\u001b[0m:\u001b[36membed_chunk\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mSuccessfully embedded 33 chunks\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedder.embed_chunk(chunks=chunked_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 14:13:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mcreating or getting the collection\u001b[0m\n",
      "\u001b[32m2026-01-31 14:13:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mgetting the embedder\u001b[0m\n",
      "\u001b[32m2026-01-31 14:13:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mLoading SentenceTransformer model: all-MiniLM-L6-v2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initializing...\n",
      "  Ollama not running!\n",
      "   Run: ollama serve\n",
      "   Then: ollama pull llama3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 14:13:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mModel loaded: 384d on cpu\u001b[0m\n",
      "\u001b[32m2026-01-31 14:13:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mclear\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mClearing collection 'technical_books'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Clearing old data from vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 14:13:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mclear\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mCleared 396 documents from collection\u001b[0m\n",
      "\u001b[32m2026-01-31 14:13:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1madding chunks to the collection\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Simple RAG ready!\n"
     ]
    }
   ],
   "source": [
    "from src.utils.config import settings\n",
    "\n",
    "print(\" Initializing...\")\n",
    "\n",
    "# Create components\n",
    "generator = OllamaGenerator(settings.llm, auto_setup=True)\n",
    "answerer = QueryAnswerer(generator)\n",
    "vector_store = ChromaStore(settings.vector_store)\n",
    "\n",
    "# IMPORTANT: Clear old data that has duplicate references\n",
    "print(\"üßπ Clearing old data from vector store...\")\n",
    "vector_store.clear()\n",
    "\n",
    "# Now ingest with the new deduplicated content\n",
    "vector_store.ingest(embch=embeddings)\n",
    "# Simple pipeline (no query enhancement)\n",
    "simple_rag = SimpleRAGPipeline(\n",
    "    vector_store=vector_store,\n",
    "    answerer=answerer\n",
    ")\n",
    "\n",
    "print(\" Simple RAG ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 14:13:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mquerying the results\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 14:13:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mfinished the querying - found 3 unique results\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SIMPLE RAG TEST\n",
      "============================================================\n",
      "üîç Searching for: What is Word2Vec?\n",
      "[SearchResult(content=\"## 1.1 Goals of the Paper\\n\\nThe main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more\\n\\n\\n\\nthan a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.\\n\\nWe use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].\\n\\nSomewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector('King') - vector('Man') + vector('Woman') results in a vector that is closest to the vector representation of the word Queen [20].\\n\\nIn this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities 1 , and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.\\n\\n\", metadata=ChunkMetadata(source_doc_title='Word2Vec', chapter_name='1.1 Goals of the Paper', page_range=(1, 2), char_span=(2477, 4299), chunk_id=UUID('83ae8b4c-1a46-44e9-b276-c4dc5b1882b6')), score=1.2118830680847168), SearchResult(content='## 7 Follow-Up Work\\n\\nAfter the initial version of this paper was written, we published single-machine multi-threaded C++ code for computing the word vectors, using both the continuous bag-of-words and skip-gram architectures 4 . The training speed is significantly higher than reported earlier in this paper, i.e. it is in the order of billions of words per hour for typical hyperparameter choices. We also published more than 1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of our follow-up work will be published in an upcoming NIPS 2013 paper [21].\\n\\n', metadata=ChunkMetadata(source_doc_title='Word2Vec', chapter_name='7 Follow-Up Work', page_range=(11, 11), char_span=(39911, 40512), chunk_id=UUID('1475c8a8-3cb6-4f15-9203-83685d4c4b69')), score=1.220699667930603), SearchResult(content='## 1.2 Previous Work\\n\\nRepresentation of words as continuous vectors has a long history [10, 26, 8]. A very popular model architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others.\\n\\nAnother interesting architecture of NNLM was presented in [13, 14], where the word vectors are first learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model.\\n\\nIt was later shown that the word vectors can be used to significantly improve and simplify many NLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word vectors were made available for future research and comparison 2 . However, as far as we know, these architectures were significantly more computationally expensive for training than the one proposed in [13], with the exception of certain version of log-bilinear model where diagonal weight matrices are used [23].\\n\\n', metadata=ChunkMetadata(source_doc_title='Word2Vec', chapter_name='1.2 Previous Work', page_range=(2, 2), char_span=(4299, 5776), chunk_id=UUID('7e4ecd55-4281-4564-a619-cbe086e8ce8c')), score=1.2496622800827026)]\n",
      "<class 'list'>\n",
      "<class 'src.shared.models.SearchResult'>\n",
      " Found 3 results\n",
      "[1] ## 1.1 Goals of the Paper\n",
      "\n",
      "The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more\n",
      "\n",
      "\n",
      "\n",
      "than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.\n",
      "\n",
      "We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].\n",
      "\n",
      "Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector('King') - vector('Man') + vector('Woman') results in a vector that is closest to the vector representation of the word Queen [20].\n",
      "\n",
      "In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities 1 , and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.\n",
      "\n",
      "\n",
      "\n",
      "[2] ## 7 Follow-Up Work\n",
      "\n",
      "After the initial version of this paper was written, we published single-machine multi-threaded C++ code for computing the word vectors, using both the continuous bag-of-words and skip-gram architectures 4 . The training speed is significantly higher than reported earlier in this paper, i.e. it is in the order of billions of words per hour for typical hyperparameter choices. We also published more than 1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of our follow-up work will be published in an upcoming NIPS 2013 paper [21].\n",
      "\n",
      "\n",
      "\n",
      "[3] ## 1.2 Previous Work\n",
      "\n",
      "Representation of words as continuous vectors has a long history [10, 26, 8]. A very popular model architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others.\n",
      "\n",
      "Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are first learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model.\n",
      "\n",
      "It was later shown that the word vectors can be used to significantly improve and simplify many NLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word vectors were made available for future research and comparison 2 . However, as far as we know, these architectures were significantly more computationally expensive for training than the one proposed in [13], with the exception of certain version of log-bilinear model where diagonal weight matrices are used [23].\n",
      "\n",
      "\n",
      "\n",
      "üìù Answer:\n",
      "The question \"What is Word2Vec?\" is not explicitly answered in the provided context. However, based on the information given, it can be inferred that Word2Vec refers to a technique or architecture for learning high-quality word vectors from huge data sets.\n",
      "\n",
      "In the context of the paper, Word2Vec seems to refer to a specific model architecture or approach for estimating neural network language models (NNLM), which involves learning word vectors using a simple model. The authors mention that they directly extend an existing architecture presented in [13, 14], where the word vectors are first learned using a neural network with a single hidden layer.\n",
      "\n",
      "It is also mentioned that Word2Vec was previously used to estimate word vectors and improve NLP applications, but it is not clear if this refers specifically to the Word2Vec algorithm or another related technique.\n"
     ]
    }
   ],
   "source": [
    "# Test simple version\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIMPLE RAG TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "answer = simple_rag.query(\"What is Word2Vec?\", top_k=3)\n",
    "\n",
    "print(\"\\nüìù Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Multi-Query Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-query RAG ready!\n"
     ]
    }
   ],
   "source": [
    "# Multi-query pipeline (with query enhancement)\n",
    "query_constructor = MultiQueryConstructor(generator)\n",
    "\n",
    "multi_rag = MultiQueryRAGPipeline(\n",
    "    query_constructor=query_constructor,\n",
    "    vector_store=vector_store,\n",
    "    answerer=answerer\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Multi-query RAG ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MULTI-QUERY RAG TEST\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 14:14:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mquerying the results\u001b[0m\n",
      "\u001b[32m2026-01-31 14:14:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mfinished the querying - found 12 unique results\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using 4 query variations\n",
      "['how are the embeddings are constructed ?', 'Here are three potential search queries related to how embeddings are constructed:', '1. \"How are word embeddings like Word2Vec and GloVe created?\"', '2. \"What is the process for constructing sentence embeddings using techniques like BERT and Sentence-BERT?\"']\n",
      " Found 12 total results\n",
      "[1] ## 7 Follow-Up Work\n",
      "\n",
      "After the initial version of this paper was written, we published single-machine multi-threaded C++ code for computing the word vectors, using both the continuous bag-of-words and skip-gram architectures 4 . The training speed is significantly higher than reported earlier in this paper, i.e. it is in the order of billions of words per hour for typical hyperparameter choices. We also published more than 1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of our follow-up work will be published in an upcoming NIPS 2013 paper [21].\n",
      "\n",
      "\n",
      "\n",
      "[2] ## 3.1 Continuous Bag-of-Words Model\n",
      "\n",
      "The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged). We call this architecture a bag-of-words model as the order of words in the history does not influence the projection. Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word. Training complexity is then\n",
      "\n",
      "$$Q = N \\times D + D \\times \\log _ { 2 } ( V ) .$$\n",
      "\n",
      "We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context. The model architecture is shown at Figure 1. Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.\n",
      "\n",
      "\n",
      "\n",
      "[3] viding us the test set.\n",
      "\n",
      "\n",
      "\n",
      "Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relationship test set, and word vectors from our models. Full vocabularies are used.\n",
      "\n",
      "| Model                 | Vector Dimensionality   | Training words   | Accuracy [%]   | Accuracy [%]   | Accuracy [%]   |\n",
      "|-----------------------|-------------------------|------------------|----------------|----------------|----------------|\n",
      "|                       |                         |                  | Semantic       | Syntactic      | Total          |\n",
      "| Collobert-Weston NNLM | 50                      | 660M             | 9.3            | 12.3           | 11.0           |\n",
      "| Turian NNLM           | 50                      | 37M              | 1.4            | 2.6            | 2.1            |\n",
      "| Turian NNLM           | 200                     | 37M              | 1.4            | 2.2            | 1.8            |\n",
      "| Mnih NNLM             | 50                      | 37M              | 1.8            | 9.1            | 5.8            |\n",
      "| Mnih NNLM             | 100                     | 37M              | 3.3            | 13.2           | 8.8            |\n",
      "| Mikolov RNNLM         | 80                      | 320M             | 4.9            | 18.4           | 12.7           |\n",
      "| Mikolov RNNLM         | 640                     | 320M             | 8.6            | 36.5           | 24.6           |\n",
      "| Huang NNLM            | 50                      | 990M             | 13.3           | 11.6           | 12.3           |\n",
      "| Our NNLM              | 20                      | 6B               | 12.9           | 26.4           | 20.3           |\n",
      "| Our NNLM              | 50                      | 6B               | 27.9           | 55.8           | 43.2           |\n",
      "| Our NNLM              | 100                     | 6B               | 34.2           | 64.5           | 50.8           |\n",
      "| CBOW                  | 300                     | 783M             | 15.5           | 53.1           | 36.1           |\n",
      "| Skip-gram        \n",
      "\n",
      "[4] ## 4.1 Task Description\n",
      "\n",
      "To measure quality of the word vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions. Two examples from each category are shown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions in each category were created in two steps: first, a list of similar word pairs was created manually. Then, a large list of questions is formed by connecting two word pairs. For example, we made a list of 68 large American cities and the states they belong to, and formed about 2.5K questions by picking two word pairs at random. We have included in our test set only single token words, thus multi-word entities are not present (such as New York ).\n",
      "\n",
      "We evaluate the overall accuracy for all question types, and for each question type separately (semantic, syntactic). Question is assumed to be correctly answered only if the closest word to the vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely to be impossible, as the current models do not have any input information about word morphology. However, we believe that usefulness of the word vectors for certain applications should be positively correlated with this accuracy metric. Further progress can be achieved by incorporating information about structure of words, especially for the syntactic questions.\n",
      "\n",
      "\n",
      "\n",
      "[5] uld be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented.\n",
      "\n",
      "\n",
      "\n",
      "Table 1: Examples of five types of semantic and nine types of syntactic questions in the SemanticSyntactic Word Relationship test set.\n",
      "\n",
      "| Type of relationship   | Word Pair 1   | Word Pair 1   | Word Pair 2   | Word Pair 2   |\n",
      "|------------------------|---------------|---------------|---------------|---------------|\n",
      "| Common capital city    | Athens        | Greece        | Oslo          | Norway        |\n",
      "| All capital cities     | Astana        | Kazakhstan    | Harare        | Zimbabwe      |\n",
      "| Currency               | Angola        | kwanza        | Iran          | rial          |\n",
      "| City-in-state          | Chicago       | Illinois      | Stockton      | California    |\n",
      "| Man-Woman              | brother       | sister        | grandson      | granddaughter |\n",
      "| Adjective to adverb    | apparent      | apparently    | rapid         | rapidly       |\n",
      "| Opposite               | possibly      | impossibly    | ethical       | unethical     |\n",
      "| Comparative            | great         | greater       | tough         | tougher       |\n",
      "| Superlative            | easy          | easiest       | lucky         | luckiest      |\n",
      "| Present Participle     | think         | thinking      | read          | reading       |\n",
      "| Nationality adjective  | Switzerland   | Swiss         | Cambodia      | Cambodian     |\n",
      "| Past tense             | walking       | walked        | swimming      | swam          |\n",
      "| Plural nouns           | mouse         | mice          | dollar        | dollars       |\n",
      "| Plural verbs           | work          | works         | speak         | speaks        |\n",
      "\n",
      "\n",
      "\n",
      "üìù Answer:\n",
      "The word vectors are constructed using a neural network language model (NNLM) architecture, specifically the Continuous Bag-of-Words (CBOW) and Skip-gram models. The CBOW model projects all words into the same position in the vector space by averaging their vectors, while the Skip-gram model uses a log-linear classifier to predict the current word based on its context.\n"
     ]
    }
   ],
   "source": [
    "# Test multi-query version\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-QUERY RAG TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "answer = multi_rag.query(\"how are the embeddings are constructed ?\", top_k=5)\n",
    "\n",
    "print(\"\\nüìù Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: Check what's in the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 14:14:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mquerying the results\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 14:14:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mfinished the querying - found 2 unique results\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store has 33 documents\n",
      "\n",
      "Found 2 results for 'word embedding'\n",
      "\n",
      "First result preview:\n",
      "## Efficient Estimation of Word Representations in Vector Space\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vector store has {vector_store.count()} documents\")\n",
    "\n",
    "# Try a simple search\n",
    "test_results = vector_store.query([\"word embedding\"], n_result=2)\n",
    "print(f\"\\nFound {len(test_results)} results for 'word embedding'\")\n",
    "\n",
    "if test_results:\n",
    "    print(\"\\nFirst result preview:\")\n",
    "    print(test_results[0].content[:200] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
