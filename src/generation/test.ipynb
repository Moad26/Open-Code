{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working RAG Pipeline\n",
    "## All Issues Fixed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = str(Path.cwd().parent.parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "import ollama\n",
    "from src.utils.config import LLMConfig, get_config\n",
    "from src.shared.models import SearchResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaManager:\n",
    "    @staticmethod\n",
    "    def ensure_ready(model_name: str = \"llama3.2\"):\n",
    "        try:\n",
    "            models = ollama.list()\n",
    "            if not any(model_name in m['name'] for m in models['models']):\n",
    "                print(f\" Downloading {model_name}...\")\n",
    "                ollama.pull(model_name)\n",
    "            print(f\" {model_name} ready\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"  Ollama not running!\")\n",
    "            print(f\"   Run: ollama serve\")\n",
    "            print(f\"   Then: ollama pull {model_name}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGenerator(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        pass\n",
    "\n",
    "class OllamaGenerator(BaseGenerator):\n",
    "    def __init__(self, config: LLMConfig, auto_setup: bool = True):\n",
    "        self.model = config.model_name\n",
    "        self.temperature = config.temperature\n",
    "        if auto_setup:\n",
    "            OllamaManager.ensure_ready(self.model)\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": self.temperature}\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryConstructor(ABC):\n",
    "    @abstractmethod\n",
    "    def refine_query(self, query: str) -> list[str]:\n",
    "        pass\n",
    "\n",
    "class MultiQueryConstructor(QueryConstructor):\n",
    "    def __init__(self, generator: BaseGenerator) -> None:\n",
    "        self.generator = generator\n",
    "        self.template = \"\"\"Generate 3 different versions of this question for better document search.\n",
    "Original question: {question}\n",
    "\n",
    "Provide 3 alternative phrasings, one per line:\"\"\"\n",
    "\n",
    "    def refine_query(self, query: str) -> list[str]:\n",
    "        prompt = self.template.format(question=query)\n",
    "        response = self.generator.generate(prompt)\n",
    "        queries = [q.strip() for q in response.split('\\n') if q.strip() and len(q.strip()) > 10]\n",
    "        return [query] + queries[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseQueryAnswerer(ABC):\n",
    "    @abstractmethod\n",
    "    def answer(self, result_search: List[SearchResult], query: str) -> str:\n",
    "        pass\n",
    "\n",
    "class QueryAnswerer(BaseQueryAnswerer):\n",
    "    def __init__(self, generator: BaseGenerator) -> None:\n",
    "        self.generator = generator\n",
    "        self.template = \"\"\"Answer this question using only the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    def answer(self, result_search: List[SearchResult], query: str) -> str:\n",
    "        if not result_search:\n",
    "            return \"No relevant documents found.\"\n",
    "        \n",
    "        context_parts = [f\"[{i}] {r.content}\" for i, r in enumerate(result_search, 1)]\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        prompt = self.template.format(context=context, question=query)\n",
    "        return self.generator.generate(prompt).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline (SIMPLE VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moad/desktop/open-books/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.ingestion.vector_store.stores import ChromaStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAGPipeline:\n",
    "    def __init__(self, vector_store: ChromaStore, answerer: BaseQueryAnswerer):\n",
    "        self.vector_store = vector_store\n",
    "        self.answerer = answerer\n",
    "    \n",
    "    def query(self, user_query: str, top_k: int = 5) -> str:\n",
    "        print(f\"üîç Searching for: {user_query}\")\n",
    "        \n",
    "        results = self.vector_store.query([user_query], n_result=top_k)\n",
    "        print(results)\n",
    "        print(type(results))\n",
    "        print(type(results[0]))\n",
    "        \n",
    "        print(f\" Found {len(results)} results\")\n",
    "        \n",
    "        answer = self.answerer.answer(results, user_query)\n",
    "        return answer\n",
    "\n",
    "\n",
    "class MultiQueryRAGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_constructor: QueryConstructor,\n",
    "        vector_store: ChromaStore, \n",
    "        answerer: BaseQueryAnswerer\n",
    "    ):\n",
    "        self.query_constructor = query_constructor\n",
    "        self.vector_store = vector_store\n",
    "        self.answerer = answerer\n",
    "    \n",
    "    def query(self, user_query: str, top_k: int = 5) -> str:\n",
    "        queries = self.query_constructor.refine_query(user_query)\n",
    "        print(f\" Using {len(queries)} query variations\")\n",
    "        \n",
    "        results = self.vector_store.query(queries, n_result=top_k)\n",
    "        \n",
    "        print(f\" Found {len(results)} total results\")\n",
    "        \n",
    "        top_results = results[:top_k]\n",
    "        \n",
    "        answer = self.answerer.answer(top_results, user_query)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Simple Version First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion.parsers.get_parser import get_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "pdf_path = Path(\"../../data/Word2Vec.pdf\")\n",
    "\n",
    "import os\n",
    "\n",
    "print(f\"File exists: {os.path.exists(pdf_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion.chunking.get_chunker import get_chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = get_chunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-30 21:10:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.parsers.parsers\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mStarting to parse PDF: ../../data/Word2Vec.pdf\u001b[0m\n",
      "\u001b[32m2026-01-30 21:10:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.parsers.parsers\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mDocument converted successfully: 12 pages\u001b[0m\n",
      "\u001b[32m2026-01-30 21:10:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.parsers.parsers\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mStructure extracted: 23 chapters\u001b[0m\n",
      "\u001b[32m2026-01-30 21:10:36\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.ingestion.parsers.parsers\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m66\u001b[0m - \u001b[32m\u001b[1mSuccessfully parsed Word2Vec.pdf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "parsed_doc = parser.parse(pdf_path=pdf_path)\n",
    "chunked_doc = chunker.chunk(parsed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion.embedding.get_embbedder import get_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-30 21:10:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mLoading SentenceTransformer model: all-MiniLM-L6-v2\u001b[0m\n",
      "\u001b[32m2026-01-30 21:10:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mModel loaded: 384d on cpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embedder = get_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-30 21:10:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.base_embed\u001b[0m:\u001b[36membed_chunk\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mSuccessfully embedded 33 chunks\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedder.embed_chunk(chunks=chunked_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-30 21:10:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mcreating or getting the collection\u001b[0m\n",
      "\u001b[32m2026-01-30 21:10:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mgetting the embedder\u001b[0m\n",
      "\u001b[32m2026-01-30 21:10:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mLoading SentenceTransformer model: all-MiniLM-L6-v2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initializing...\n",
      "  Ollama not running!\n",
      "   Run: ollama serve\n",
      "   Then: ollama pull llama3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-30 21:10:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mModel loaded: 384d on cpu\u001b[0m\n",
      "\u001b[32m2026-01-30 21:10:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1madding chunks to the collection\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Simple RAG ready!\n"
     ]
    }
   ],
   "source": [
    "from src.utils.config import settings\n",
    "\n",
    "print(\" Initializing...\")\n",
    "\n",
    "# Create components\n",
    "generator = OllamaGenerator(settings.llm, auto_setup=True)\n",
    "answerer = QueryAnswerer(generator)\n",
    "vector_store = ChromaStore(settings.vector_store)\n",
    "vector_store.ingest(embch=embeddings)\n",
    "# Simple pipeline (no query enhancement)\n",
    "simple_rag = SimpleRAGPipeline(\n",
    "    vector_store=vector_store,\n",
    "    answerer=answerer\n",
    ")\n",
    "\n",
    "print(\" Simple RAG ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-30 21:10:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mquerying the results\u001b[0m\n",
      "\u001b[32m2026-01-30 21:10:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mfinished the querying - found 3 unique results\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SIMPLE RAG TEST\n",
      "============================================================\n",
      "üîç Searching for: What is Word2Vec?\n",
      "[SearchResult(content=\"## 1.1 Goals of the Paper\\n\\nThe main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more\\n\\n\\n\\nthan a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.\\n\\nWe use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].\\n\\nSomewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector('King') - vector('Man') + vector('Woman') results in a vector that is closest to the vector representation of the word Queen [20].\\n\\nIn this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities 1 , and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.\\n\\n\", metadata=ChunkMetadata(source_doc_title='Word2Vec', chapter_name='1.1 Goals of the Paper', page_range=(1, 2), char_span=(2477, 4299), chunk_id=UUID('0278f48b-14c8-4aa8-a776-0fe555da8644')), score=1.2118830680847168), SearchResult(content=\"## 1.1 Goals of the Paper\\n\\nThe main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more\\n\\n\\n\\nthan a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.\\n\\nWe use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].\\n\\nSomewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector('King') - vector('Man') + vector('Woman') results in a vector that is closest to the vector representation of the word Queen [20].\\n\\nIn this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities 1 , and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.\\n\\n\", metadata=ChunkMetadata(source_doc_title='Word2Vec', chapter_name='1.1 Goals of the Paper', page_range=(1, 2), char_span=(2477, 4299), chunk_id=UUID('da81b9d8-18e5-4bc5-88fb-73ba8cfade9f')), score=1.2118830680847168), SearchResult(content=\"## 1.1 Goals of the Paper\\n\\nThe main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more\\n\\n\\n\\nthan a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.\\n\\nWe use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].\\n\\nSomewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector('King') - vector('Man') + vector('Woman') results in a vector that is closest to the vector representation of the word Queen [20].\\n\\nIn this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities 1 , and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.\\n\\n\", metadata=ChunkMetadata(source_doc_title='Word2Vec', chapter_name='1.1 Goals of the Paper', page_range=(1, 2), char_span=(2477, 4299), chunk_id=UUID('2eeea6ed-0011-40f6-aaae-caf8af4cbd6b')), score=1.2118830680847168)]\n",
      "<class 'list'>\n",
      "<class 'src.shared.models.SearchResult'>\n",
      " Found 3 results\n",
      "\n",
      "üìù Answer:\n",
      "The question cannot be answered based on the provided context. The text does not mention \"Word2Vec\" explicitly. However, it mentions a technique called \"word offset,\" which is related to Word2Vec, as it involves performing simple algebraic operations on word vectors to find similar words.\n"
     ]
    }
   ],
   "source": [
    "# Test simple version\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIMPLE RAG TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "answer = simple_rag.query(\"What is Word2Vec?\", top_k=3)\n",
    "\n",
    "print(\"\\nüìù Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Multi-Query Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-query RAG ready!\n"
     ]
    }
   ],
   "source": [
    "# Multi-query pipeline (with query enhancement)\n",
    "query_constructor = MultiQueryConstructor(generator)\n",
    "\n",
    "multi_rag = MultiQueryRAGPipeline(\n",
    "    query_constructor=query_constructor,\n",
    "    vector_store=vector_store,\n",
    "    answerer=answerer\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Multi-query RAG ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MULTI-QUERY RAG TEST\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-30 21:10:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mquerying the results\u001b[0m\n",
      "\u001b[32m2026-01-30 21:10:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mfinished the querying - found 12 unique results\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using 4 query variations\n",
      " Found 12 total results\n",
      "\n",
      "üìù Answer:\n",
      "The question cannot be answered based on the provided context. The text does not mention Word2Vec explicitly. However, it appears to be related to a paper discussing techniques for learning high-quality word vectors from large datasets, which might be similar to Word2Vec.\n"
     ]
    }
   ],
   "source": [
    "# Test multi-query version\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-QUERY RAG TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "answer = multi_rag.query(\"What is Word2Vec?\", top_k=5)\n",
    "\n",
    "print(\"\\nüìù Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: Check what's in the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-30 21:10:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mquerying the results\u001b[0m\n",
      "\u001b[32m2026-01-30 21:10:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mfinished the querying - found 2 unique results\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store has 231 documents\n",
      "\n",
      "Found 2 results for 'word embedding'\n",
      "\n",
      "First result preview:\n",
      "## Efficient Estimation of Word Representations in Vector Space\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vector store has {vector_store.count()} documents\")\n",
    "\n",
    "# Try a simple search\n",
    "test_results = vector_store.query([\"word embedding\"], n_result=2)\n",
    "print(f\"\\nFound {len(test_results)} results for 'word embedding'\")\n",
    "\n",
    "if test_results:\n",
    "    print(\"\\nFirst result preview:\")\n",
    "    print(test_results[0].content[:200] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
