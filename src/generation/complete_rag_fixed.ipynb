{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete RAG Pipeline with Ollama\n",
    "## Fixed and Ready to Use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup project path\n",
    "project_root = str(Path.cwd().parent.parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "import ollama\n",
    "from src.utils.config import LLMConfig, get_config\n",
    "from src.shared.models import SearchResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Ollama Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaManager:\n",
    "    @staticmethod\n",
    "    def ensure_ready(model_name: str = \"llama3.2\"):\n",
    "        \"\"\"Ensure Ollama server is running and model is available\"\"\"\n",
    "        try:\n",
    "            # Check if server is running\n",
    "            models = ollama.list()\n",
    "            \n",
    "            # Check if model exists\n",
    "            if not any(model_name in m['name'] for m in models['models']):\n",
    "                print(f\"üì• Downloading {model_name} (this may take a few minutes)...\")\n",
    "                ollama.pull(model_name)\n",
    "                print(f\"‚úÖ Model {model_name} downloaded successfully\")\n",
    "            else:\n",
    "                print(f\"‚úÖ Model {model_name} is ready\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Ollama not running!\")\n",
    "            print(f\"   Please run in a terminal: ollama serve\")\n",
    "            print(f\"   Then pull the model: ollama pull {model_name}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generator Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGenerator(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate text from prompt\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class OllamaGenerator(BaseGenerator):\n",
    "    def __init__(self, config: LLMConfig, auto_setup: bool = True):\n",
    "        self.model = config.model_name\n",
    "        self.temperature = config.temperature\n",
    "        \n",
    "        if auto_setup:\n",
    "            OllamaManager.ensure_ready(self.model)\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": self.temperature}\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error: {e}\\nIs Ollama running? Run 'ollama serve'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryConstructor(ABC):\n",
    "    @abstractmethod\n",
    "    def refine_query(self, query: str) -> list[str]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class MultiQueryConstructor(QueryConstructor):\n",
    "    def __init__(self, generator: BaseGenerator) -> None:\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines,And give just the questions nothing else. Original question: {question}\"\"\"\n",
    "\n",
    "    def refine_query(self, query: str) -> list[str]:\n",
    "        prompt = self.template.format(question=query)\n",
    "        response = self.generator.generate(prompt)\n",
    "        \n",
    "        # Parse response into individual queries\n",
    "        queries = [q.strip() for q in response.split('\\n') if q.strip()]\n",
    "        \n",
    "        # Include original query\n",
    "        return [query] + queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseQueryAnswerer(ABC):\n",
    "    @abstractmethod\n",
    "    def answer(self, result_search: List[SearchResult], query: str) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class QueryAnswerer(BaseQueryAnswerer):\n",
    "    def __init__(self, generator: BaseGenerator) -> None:\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.template = \"\"\"You are a helpful assistant answering questions based on provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Answer based only on the provided context\n",
    "- If the context doesn't contain enough information, say so\n",
    "- Be concise but complete\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    def answer(self, result_search: List[SearchResult], query: str) -> str:\n",
    "        if not result_search:\n",
    "            return \"No relevant documents found to answer this question.\"\n",
    "        \n",
    "        # Build context from search results\n",
    "        context_parts = []\n",
    "        for i, result in enumerate(result_search, 1):\n",
    "            context_parts.append(f\"[Document {i}]\\n{result.content}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt = self.template.format(context=context, question=query)\n",
    "        answer = self.generator.generate(prompt)\n",
    "        \n",
    "        return answer.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG Pipeline (FIXED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moad/desktop/open-books/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.ingestion.vector_store.stores import ChromaStore\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_constructor: QueryConstructor,\n",
    "        vector_store: ChromaStore, \n",
    "        answerer: BaseQueryAnswerer\n",
    "    ):\n",
    "        self.query_constructor = query_constructor\n",
    "        self.vector_store = vector_store\n",
    "        self.answerer = answerer\n",
    "    \n",
    "    def query(self, user_query: str, top_k: int = 5) -> str:\n",
    "        # Get enhanced queries\n",
    "        enhanced_queries = self.query_constructor.refine_query(user_query)\n",
    "        print(f\"üîç Generated {len(enhanced_queries)} query variations\")\n",
    "        print(f\"the queries are {enhanced_queries}\")\n",
    "        \n",
    "        # Query returns List[List[SearchResult]] - one list per query\n",
    "        results_nested = self.vector_store.query_flattened(enhanced_queries, n_result=top_k)\n",
    "        \n",
    "        # Flatten the nested list (THIS WAS THE BUG!)\n",
    "        all_results = []\n",
    "        for query_results in results_nested:\n",
    "            all_results.extend(query_results)\n",
    "        \n",
    "        print(f\"üìÑ Retrieved {len(all_results)} total results\")\n",
    "        print(results_nested)\n",
    "        \n",
    "        # Take top_k results\n",
    "        top_results = all_results[:top_k]\n",
    "        \n",
    "        # Generate answer\n",
    "        print(f\"üí≠ Generating answer...\")\n",
    "        answer = self.answerer.answer(top_results, user_query)\n",
    "        \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the Complete System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  IMPORTANT: Make sure you've updated config/config.yaml to use Ollama!\n",
      "    See the cell above for the required changes.\n",
      "\n",
      "üìã Loading configuration...\n"
     ]
    }
   ],
   "source": [
    "# First, update your config to use Ollama\n",
    "# You need to manually edit config/config.yaml and change:\n",
    "# llm:\n",
    "#   provider: ollama\n",
    "#   model_name: llama3.2\n",
    "#   base_url: http://localhost:11434\n",
    "#   temperature: 0.7\n",
    "\n",
    "print(\"‚ö†Ô∏è  IMPORTANT: Make sure you've updated config/config.yaml to use Ollama!\")\n",
    "print(\"    See the cell above for the required changes.\")\n",
    "print(\"\\nüìã Loading configuration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 22:59:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mcreating or getting the collection\u001b[0m\n",
      "\u001b[32m2026-01-29 22:59:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mgetting the embedder\u001b[0m\n",
      "\u001b[32m2026-01-29 22:59:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mLoading SentenceTransformer model: all-MiniLM-L6-v2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provider='ollama' model_name='llama3.2' api_key=None base_url=None temperature=0.1\n",
      "ü§ñ Initializing Ollama generator...\n",
      "‚ö†Ô∏è  Ollama not running!\n",
      "   Please run in a terminal: ollama serve\n",
      "   Then pull the model: ollama pull llama3.2\n",
      "üîß Setting up RAG components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 22:59:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.embedding.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mModel loaded: 384d on cpu\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "from src.utils.config import settings\n",
    "print(settings.llm)\n",
    "# Create generator\n",
    "print(\"ü§ñ Initializing Ollama generator...\")\n",
    "generator = OllamaGenerator(settings.llm, auto_setup=True)\n",
    "\n",
    "# Create components\n",
    "print(\"üîß Setting up RAG components...\")\n",
    "query_constructor = MultiQueryConstructor(generator)\n",
    "answerer = QueryAnswerer(generator)\n",
    "vector_store = ChromaStore(settings.vector_store)\n",
    "\n",
    "# Create pipeline\n",
    "rag = RAGPipeline(\n",
    "    query_constructor=query_constructor,\n",
    "    vector_store=vector_store,\n",
    "    answerer=answerer\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG pipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing RAG Pipeline\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-29 22:59:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery_flattened\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mquerying the results\u001b[0m\n",
      "\u001b[32m2026-01-29 22:59:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery_flattened\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mfinished the querying\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Generated 6 query variations\n",
      "the queries are ['Talk about Word to vector implementation ?', 'How does Word2Vec work in a vector database?', 'What are the key concepts and techniques behind Word2Vec?', 'Can you explain the Word2Vec algorithm and its applications in natural language processing?', 'How does Word2Vec handle out-of-vocabulary words in a vector database?', 'What are some common use cases for Word2Vec in text analysis and information retrieval?']\n",
      "üìÑ Retrieved 0 total results\n",
      "[]\n",
      "üí≠ Generating answer...\n",
      "\n",
      "============================================================\n",
      "ANSWER:\n",
      "============================================================\n",
      "No relevant documents found to answer this question.\n"
     ]
    }
   ],
   "source": [
    "# Test it!\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing RAG Pipeline\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "answer = rag.query(\"Talk about Word to vector implementation ?\", top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\"*60)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Simple Query (No RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple query test:\n",
      "RAG (Rescue and Adoption Group) typically refers to a non-profit organization that specializes in rescuing and rehoming animals, often with a focus on specific breeds or types of pets.\n"
     ]
    }
   ],
   "source": [
    "# Test generator directly\n",
    "simple_response = generator.generate(\"Explain what RAG is in one sentence.\")\n",
    "print(\"Simple query test:\")\n",
    "print(simple_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
