# Efficient Estimation of Word Representations in Vector Space

# Tomas Mikolov

# Abstract

# 1 Introduction

# 1.1 Goals of the Paper

# 1.2 Previous Work

# 2 Model Architectures

# 2.1 Feedforward Neural Net Language Model (NNLM)

# 2.2 Recurrent Neural Net Language Model (RNNLM)

# 2.3 Parallel Training of Neural Networks

# 3 New Log-linear Models

# 3.1 Continuous Bag-of-Words Model

# 3.2 Continuous Skip-gram Model

# 4 Results

# 4.1 Task Description

# 4.2 Maximization of Accuracy

# 4.3 Comparison of Model Architectures

# 4.4 Large Scale Parallel Training of Models

# 4.5 Microsoft Research Sentence Completion Challenge

# 5 Examples of the Learned Relationships

# 6 Conclusion

# 7 Follow-Up Work

# References
