{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Manager Test - With Progress Tracking\n",
    "\n",
    "This notebook tests the LibraryManager with better progress visibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root: /home/moad/desktop/open-books\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = str(Path.cwd().parent.parent.parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"‚úÖ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import LibraryManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "from src.utils.config import settings, LibreryConfig\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Library management for syncing PDFs to vector store.\"\"\"\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "from src.ingestion.chunking.get_chunker import get_chunker\n",
    "from src.ingestion.embedding.get_embbedder import get_embedder\n",
    "from src.ingestion.parsers.get_parser import get_parser\n",
    "from src.ingestion.vector_store.stores import get_store, ChromaStore\n",
    "from src.utils.logger import logger\n",
    "\n",
    "\n",
    "class LibraryManager:\n",
    "    \"\"\"Manages PDF library syncing with vector store.\"\"\"\n",
    "\n",
    "    def __init__(self, config: LibreryConfig) -> None:\n",
    "        self.books_dir = Path(config.books_paths)\n",
    "        self.manifest_path = Path(config.manifest_path)\n",
    "\n",
    "        logger.info(\"Initializing LibraryManager...\")\n",
    "\n",
    "        # Initialize components (embedder loaded once here)\n",
    "        self.store = ChromaStore(settings.vector_store)\n",
    "        self.parser = get_parser()\n",
    "        self.chunker = get_chunker()\n",
    "\n",
    "        # Load manifest\n",
    "        self.manifest = self._load_manifest()\n",
    "        logger.info(f\"Loaded manifest with {len(self.manifest)} entries\")\n",
    "\n",
    "    def _load_manifest(self) -> Dict[str, str]:\n",
    "        \"\"\"Load manifest file or create empty one.\"\"\"\n",
    "        if self.manifest_path.exists():\n",
    "            try:\n",
    "                return json.loads(self.manifest_path.read_text())\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load manifest: {e}. Starting fresh.\")\n",
    "                return {}\n",
    "        return {}\n",
    "\n",
    "    def _save_manifest(self) -> None:\n",
    "        \"\"\"Save manifest to disk.\"\"\"\n",
    "        try:\n",
    "            self.manifest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            self.manifest_path.write_text(json.dumps(self.manifest, indent=2))\n",
    "            logger.debug(\"Manifest saved\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save manifest: {e}\")\n",
    "\n",
    "    def _calculate_hash(self, file_path: Path) -> str:\n",
    "        \"\"\"Calculate SHA256 hash of file.\"\"\"\n",
    "        sha256 = hashlib.sha256()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            while chunk := f.read(8192):\n",
    "                sha256.update(chunk)\n",
    "        return sha256.hexdigest()\n",
    "\n",
    "    def sync(self) -> None:\n",
    "        \"\"\"Sync books directory with vector store.\"\"\"\n",
    "        logger.info(f\"Starting sync from: {self.books_dir}\")\n",
    "\n",
    "        # Get current PDF files\n",
    "        current_files = list(self.books_dir.glob(\"*.pdf\"))\n",
    "        logger.info(f\"Found {len(current_files)} PDF files\")\n",
    "\n",
    "        if not current_files:\n",
    "            logger.warning(\"No PDF files found in books directory\")\n",
    "            return\n",
    "\n",
    "        found_filenames = {f.name for f in current_files}\n",
    "\n",
    "        self._cleanup_deleted_files(found_filenames)\n",
    "\n",
    "        self._process_files(current_files)\n",
    "\n",
    "        self._save_manifest()\n",
    "\n",
    "        logger.success(f\"Sync complete! Vector store has {self.store.count()} chunks\")\n",
    "\n",
    "    def _cleanup_deleted_files(self, found_filenames: set) -> None:\n",
    "        for filename in list(self.manifest.keys()):\n",
    "            if filename not in found_filenames:\n",
    "                logger.info(f\"File removed: {filename}\")\n",
    "                try:\n",
    "                    self.store.delete_by_filename(filename)\n",
    "                    del self.manifest[filename]\n",
    "                    logger.info(f\"Cleaned up {filename} from index\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to clean up {filename}: {e}\")\n",
    "\n",
    "    def _process_files(self, current_files: list) -> None:\n",
    "        total = len(current_files)\n",
    "\n",
    "        for idx, file_path in enumerate(current_files, 1):\n",
    "            name = file_path.name\n",
    "            logger.info(f\"\\n[{idx}/{total}] Processing: {name}\")\n",
    "\n",
    "            try:\n",
    "                # Calculate current hash\n",
    "                logger.debug(f\"Calculating hash for {name}...\")\n",
    "                current_hash = self._calculate_hash(file_path)\n",
    "\n",
    "                # Check if file changed\n",
    "                if self.manifest.get(name) == current_hash:\n",
    "                    logger.info(f\"Skipping {name} (unchanged)\")\n",
    "                    continue\n",
    "\n",
    "                # File is new or changed\n",
    "                if name in self.manifest:\n",
    "                    logger.info(f\"Content changed: {name}\")\n",
    "                    self.store.delete_by_filename(name)\n",
    "                else:\n",
    "                    logger.info(f\"New file: {name}\")\n",
    "\n",
    "                # Process file\n",
    "                self._index_file(file_path, name, current_hash)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process {name}: {e}\")\n",
    "                logger.exception(\"Full traceback:\")\n",
    "                # Continue with next file instead of crashing\n",
    "\n",
    "    def _index_file(self, file_path: Path, name: str, file_hash: str) -> None:\n",
    "        # Parse\n",
    "        logger.info(f\"  Parsing {name}...\")\n",
    "        parsed_doc = self.parser.parse(file_path)\n",
    "        logger.info(f\"  Parsed {parsed_doc.metadata.nbr_pages} pages\")\n",
    "\n",
    "        # Chunk\n",
    "        logger.info(f\"  Chunking {name}...\")\n",
    "        chunked_doc = self.chunker.chunk(parsed_doc)\n",
    "        logger.info(f\"  Created {len(chunked_doc)} chunks\")\n",
    "\n",
    "        \n",
    "\n",
    "        # Ingest\n",
    "        logger.info(f\"  Storing {name}...\")\n",
    "        self.store.ingest(chunks=chunked_doc)\n",
    "        logger.info(f\"  Stored in vector DB\")\n",
    "\n",
    "        # Update manifest\n",
    "        self.manifest[name] = file_hash\n",
    "        logger.success(f\"Successfully indexed: {name}\")\n",
    "\n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get library statistics.\"\"\"\n",
    "        return {\n",
    "            \"indexed_files\": len(self.manifest),\n",
    "            \"total_chunks\": self.store.count(),\n",
    "        }\n",
    "\n",
    "    def force_reindex(self, filename: str) -> None:\n",
    "        \"\"\"Force reindex a specific file.\"\"\"\n",
    "        logger.info(f\"Force reindexing: {filename}\")\n",
    "\n",
    "        file_path = self.books_dir / filename\n",
    "        if not file_path.exists():\n",
    "            logger.error(f\"File not found: {filename}\")\n",
    "            return\n",
    "\n",
    "        # Remove from index\n",
    "        if filename in self.manifest:\n",
    "            self.store.delete_by_filename(filename)\n",
    "            del self.manifest[filename]\n",
    "\n",
    "        # Reindex\n",
    "        current_hash = self._calculate_hash(file_path)\n",
    "        self._index_file(file_path, filename, current_hash)\n",
    "        self._save_manifest()\n",
    "\n",
    "    def clear_all(self) -> None:\n",
    "        \"\"\"Clear all indexed data.\"\"\"\n",
    "        logger.warning(\"Clearing all indexed data...\")\n",
    "        self.store.clear()\n",
    "        self.manifest = {}\n",
    "        self._save_manifest()\n",
    "        logger.success(\"All data cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Books directory: /home/moad/desktop/open-books/books\n",
      "üìÑ Manifest path: /home/moad/desktop/open-books/config/manifest.json\n",
      "‚úÖ Found 2 PDF files:\n",
      "   - 1912_lora_low_rank_adaptation_of_la.pdf\n",
      "   - Adam A Method for Stochastic Optimization.pdf\n"
     ]
    }
   ],
   "source": [
    "# Verify config\n",
    "config = settings.librery\n",
    "\n",
    "print(f\"üìÅ Books directory: {config.books_paths}\")\n",
    "print(f\"üìÑ Manifest path: {config.manifest_path}\")\n",
    "\n",
    "# Check if directory exists\n",
    "books_path = Path(config.books_paths)\n",
    "if books_path.exists():\n",
    "    pdf_files = list(books_path.glob(\"*.pdf\"))\n",
    "    print(f\"‚úÖ Found {len(pdf_files)} PDF files:\")\n",
    "    for f in pdf_files:\n",
    "        print(f\"   - {f.name}\")\n",
    "else:\n",
    "    print(f\"‚ùå Directory not found: {books_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LibraryManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mInitializing LibraryManager...\u001b[0m\n",
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mcreating or getting the collection\u001b[0m\n",
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mgetting the embedder\u001b[0m\n",
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mLoaded manifest with 2 entries\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LibraryManager...\n",
      "\n",
      "\n",
      "‚úÖ LibraryManager initialized!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing LibraryManager...\\n\")\n",
    "\n",
    "manager = LibraryManager(config)\n",
    "\n",
    "print(\"\\n‚úÖ LibraryManager initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Current Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Current Stats:\n",
      "   Indexed files: 2\n",
      "   Total chunks: 304\n"
     ]
    }
   ],
   "source": [
    "stats = manager.get_stats()\n",
    "\n",
    "print(\"üìä Current Stats:\")\n",
    "print(f\"   Indexed files: {stats['indexed_files']}\")\n",
    "print(f\"   Total chunks: {stats['total_chunks']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Sync Library (Incremental)\n",
    "\n",
    "This will:\n",
    "- Skip unchanged files\n",
    "- Index new files\n",
    "- Re-index changed files\n",
    "- Remove deleted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msync\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mStarting sync from: /home/moad/desktop/open-books/books\u001b[0m\n",
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msync\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mFound 2 PDF files\u001b[0m\n",
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_process_files\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1m\n",
      "[1/2] Processing: 1912_lora_low_rank_adaptation_of_la.pdf\u001b[0m\n",
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_process_files\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mSkipping 1912_lora_low_rank_adaptation_of_la.pdf (unchanged)\u001b[0m\n",
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_process_files\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1m\n",
      "[2/2] Processing: Adam A Method for Stochastic Optimization.pdf\u001b[0m\n",
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_process_files\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mSkipping Adam A Method for Stochastic Optimization.pdf (unchanged)\u001b[0m\n",
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msync\u001b[0m:\u001b[36m80\u001b[0m - \u001b[32m\u001b[1mSync complete! Vector store has 304 chunks\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting incremental sync...\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "‚úÖ Sync complete!\n",
      "\n",
      "üìä Updated Stats:\n",
      "   Indexed files: 2\n",
      "   Total chunks: 304\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting incremental sync...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "manager.sync()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ Sync complete!\")\n",
    "\n",
    "# Show updated stats\n",
    "stats = manager.get_stats()\n",
    "print(f\"\\nüìä Updated Stats:\")\n",
    "print(f\"   Indexed files: {stats['indexed_files']}\")\n",
    "print(f\"   Total chunks: {stats['total_chunks']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Force Reindex Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to force reindex a specific file\n",
    "# manager.force_reindex(\"Word2Vec.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Clear All Data (‚ö†Ô∏è Destructive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clear all indexed data\n",
    "# manager.clear_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mquerying the results\u001b[0m\n",
      "\u001b[32m2026-01-31 20:35:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.ingestion.vector_store.stores\u001b[0m:\u001b[36mquery\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mfinished the querying - found 3 unique results\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Search results for: 'word embeddings'\n",
      "\n",
      "[1] Score: 0.7569\n",
      "    File: BERT Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "    Preview: ## 2.1 Unsupervised Feature-based Approaches\n",
      "\n",
      "Learning widely applicable representations of words has been an active area of research for decades, inc...\n",
      "\n",
      "[2] Score: 0.8421\n",
      "    File: BERT Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "    Preview: arned in translation: Contextualized word vectors. In NIPS .\n",
      "- Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic cont...\n",
      "\n",
      "[3] Score: 1.0083\n",
      "    File: BERT Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "    Preview: ## A.2 Pre-training Procedure\n",
      "\n",
      "To generate each training input sequence, we sample two spans of text from the corpus, which we refer to as 'sentences'...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test if indexing worked\n",
    "test_query = \"word embeddings\"\n",
    "results = manager.store.query([test_query], n_result=3)\n",
    "\n",
    "print(f\"üîç Search results for: '{test_query}'\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"[{i}] Score: {result.score:.4f}\")\n",
    "    print(f\"    File: {result.metadata.source_doc_title}\")\n",
    "    print(f\"    Preview: {result.content[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: View Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Current Manifest:\n",
      "{\n",
      "  \"1912_lora_low_rank_adaptation_of_la.pdf\": \"6154f901b7873cfc910e5d57156907f7b776f83f0c5f532a6b721bc614f0be35\",\n",
      "  \"Adam A Method for Stochastic Optimization.pdf\": \"eab9c73ae2ceda884b94830bda99312254bac4806f6c9f045cbab90721ecda31\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"üìã Current Manifest:\")\n",
    "print(json.dumps(manager.manifest, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
